{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TU6hPdK2EpKP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0DhcYrYFDBf",
        "outputId": "c1e0ff24-6532-4796-ddfb-7d6906931670"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gH88cGZIFEZO",
        "outputId": "87a732b9-2189-4fef-b3f7-4fa617ce662f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJeIocYSFeIX",
        "outputId": "cab03035-d820-4206-9aa9-c9969e21cca8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7A57YydqFeUz",
        "outputId": "1abb0fd9-dc3f-4e37-85cf-800445097531"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-St3mnxFghr",
        "outputId": "abc6cfd9-ded3-414a-99a4-1e7dbf56aee2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "7isp0VEeFsCD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZKWHO6XF4dl",
        "outputId": "ae84cc8f-a120-49c2-f57d-a511e16f82b1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "bA-9rRxFF6p_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWERVIk3F-lz",
        "outputId": "f492ce4b-8e32-4f9b-9483-eaac82d702ab"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99r9VRZjGAau",
        "outputId": "94da6b19-1e93-4ddb-bea4-5f360d41229c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "tRwnxCz3GDRK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7uswcvhGFmj",
        "outputId": "cdd5e6f9-adab-42bf-a1e0-24d24ef2eda1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "OwnL2uEMGLml"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5VJ3ERxGSG6",
        "outputId": "13d1af7d-861a-47e6-847f-939d69b2e554"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "metadata": {
        "id": "kfDbIRvbGW77"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ui3rTyL5Ga7S",
        "outputId": "5156847d-9cf0-47ef-fef3-72684a9e48c8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URhe7Km1Gcrd",
        "outputId": "d57a323d-dba0-4e11-9a69-77fdc8952200"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "4optenSTGd_1"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lxq4uuMyGo4g",
        "outputId": "8c1a2af0-eaf8-4923-8bbe-4fc0ea3357a8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "JBiATOA7Gqhr"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHNekWEeGr6k",
        "outputId": "69b59d7f-ddc1-43d1-b68c-4180e36929a2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrP8Iw_ZHBOR",
        "outputId": "8868ac59-35cc-4ad2-f3fd-1b7fc983d73c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "2pvU0fxaHD9P"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "qJQB71rsHGBd"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "IPSka7fDHHyU"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXQkSIK6HJHu",
        "outputId": "a0a5c800-0a64-49b3-9f74-8ad8280f8c2d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IW8ego5FHKwd",
        "outputId": "a4cb8427-ba63-4d16-c8ae-e6cce92df7cc"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "3PUANq7sHMXK"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zT9KNK4HHN7s",
        "outputId": "9f31f55e-d619-499c-8b8e-f2074a82a760"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([33, 55,  6, 45,  5, 38, 41, 34, 42, 15, 10,  1,  0, 65, 12, 15, 45,\n",
              "       22, 33, 25, 60, 12, 27, 46, 23, 46, 48, 30, 16, 24,  3, 55, 13,  4,\n",
              "       37, 61, 16, 35, 64, 56,  5,  3, 52, 51, 49, 47,  0, 57, 35, 23, 52,\n",
              "       24, 43, 64,  5, 41, 39, 50, 64, 20, 57, 38, 19,  7,  7,  0, 65, 58,\n",
              "       50, 41, 20, 17, 38, 63, 26, 44, 21, 50,  9, 19, 44, 22, 65, 40, 59,\n",
              "       61, 51, 38, 31,  3, 11, 15, 24, 22, 34, 25, 24, 10, 56, 62])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Af3tJkwyHSBP",
        "outputId": "07175dd5-3935-4c6c-fc74-defbe4671048"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'eper;\\nNor no without-book prologue, faintly spoke\\nAfter the prompter, for our entrance:\\nBut let them'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"Tp'f&YbUcB3\\n[UNK]z;BfITLu;NgJgiQCK!p?$XvCVyq&!mljh[UNK]rVJmKdy&bZkyGrYF,,[UNK]zskbGDYxMeHk.FeIzatvlYR!:BKIULK3qw\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "Ti6fVamnHTE7"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mke6MiIHWuq",
        "outputId": "21d6e054-e8b3-482b-d6b4-c7bb2f109ecc"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1888056, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45vpIrmbHaTY",
        "outputId": "a46501a1-cbdd-41ae-8084-9574b080ea32"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.943985"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "PlAYQ2aNHbp8"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "5nhj8tBCHdh8"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "xmQK8xtzHgHU"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTA2AbcjHjg1",
        "outputId": "dd48a23a-bb76-4ba7-b8d9-c3e00aa67111"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 17s 56ms/step - loss: 2.7381\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 2.0011\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 12s 55ms/step - loss: 1.7197\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.5560\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 12s 55ms/step - loss: 1.4547\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 12s 56ms/step - loss: 1.3867\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 13s 57ms/step - loss: 1.3346\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 14s 57ms/step - loss: 1.2901\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.2504\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.2114\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.1729\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 12s 56ms/step - loss: 1.1328\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 13s 58ms/step - loss: 1.0899\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.0451\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 0.9973\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 0.9472\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.8952\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 12s 56ms/step - loss: 0.8442\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 0.7919\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.7426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "569M9AofHocm"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "7MyJIdKtHsTg"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNkgaX2jHuGg",
        "outputId": "1d4e8551-34a9-4dde-abfa-f1845db1d4fe"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "How durst you, garlic, will you have begun!\n",
            "But far in that he be a mallied.\n",
            "\n",
            "MARIANA:\n",
            "I am very fair or women's; that to ear'd for arms\n",
            "He has poor'st, or't brings and braw:\n",
            "So thrive I, that have weigh'd your blusters.\n",
            "Sound that thou gives not that in himself that fladged\n",
            "and soldiers such and self-seeking on the earth,\n",
            "And do not weak pricking me at disadvenguate,\n",
            "Sicinius, that name's report when I parted with\n",
            "his eyes: which had it shall be so!\n",
            "\n",
            "LEONTES:\n",
            "Not so;\n",
            "This hand, the tears that shall never gate\n",
            "Was full three quarrels, and true-lord\n",
            "And through him sends so here you are so simple to the\n",
            "entertainment and when I make it well:\n",
            "My quick conveyance for judges cause;\n",
            "Within this kind cowardly post worse:\n",
            "For it is now elochedo deceived power,\n",
            "To stir him in extremes, and their thanks\n",
            "That you make doebly be nothing e'er the nert,\n",
            "And slept are slip upon him; he had some fill thine ear\n",
            "hath given them 'vought with death, pale fair an ague.\n",
            "A cag Belong thou dost, and then I  \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.983915328979492\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWsHGL8aHx66",
        "outputId": "b78043c0-7e3c-497e-b0ee-dd8a587ccad5"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nWell, give me thyself, that was my women so vengeance to\\nwash our ten times lig thee to her Nervice.\\n\\nBAPTISTA:\\n\\nKING EDWARD IV:\\nWhat noise is if thus to her that art women\\nof the badient of death that virtues thanks,\\nWhere your heart as we had lodged with a\\nreary with you.\\n\\nSICINIUS:\\nGo, tell me thou think'st thou me of\\nthe town. last thou, tapk, that have resisted cheeks since\\nI suppose ne'er the name o' the book:\\nMy thousand 'dward's wit, and that: thou me\\nWhose duty is sep death to she knows on me;\\nThat in no child, but makes the spirits\\nOr a three away and harm alliance:\\nThe impinion of an air which I keep from Mantua.\\n\\nBAPTISTA:\\nNay, no; I am gone enough alone;\\nLike to a mastles here,--you that thou art, I do believe\\nHer eye beat's all her deadly enemy,\\nMy maricians have no poor enemy\\nThat you are doubted; and, for I know, if thou diest'st,\\nuntiuted him in no travell'd and what's all harm blood.\\n\\nSecond Messenger:\\nAy.\\n\\nLUCENTES:\\nAy, and too slain, that Edward is enough,\\nTo great\"\n",
            " b\"ROMEO:\\n'Tis married tears to visit them;\\nThe king's shame--when they lay their faces\\nAnd thither will lose unto thy black tear;\\nAnd join, entertain, old, when they lie\\nShould do my storitity, that you may\\nAll wife cannot make it as his life.\\n\\nJOHN OF GAUNT:\\nNo, his offences shall I stook behind; 'While we proceed\\nat it; unarvied wanton stones, but that the one anointed\\nflight, what man the sin 'Be slay; and all your houses,\\nThe ripe of the Antinous done they were another,\\nAnd come in Bagoasion; and, tears, I mean too let forth\\nBROKE Voluntayon! that I may belone\\nMen hast thou judsmest me From Caliban must\\nAnd stand for such self-sent and strange against\\nThe tlung you was the fartherous eye\\nRed souls to him in died. An't please your pates,\\nTo counterfeit Myssecualer than the sin\\nWas with the danger and down for your virtues.\\nCould have find that hear me speak.\\n\\nCoRIOLANUS:\\nNo, to him aside?\\n\\nSecond Musician:\\nWhich is the walls of heart\\nPrince is vanity. Come, good Camillo,\\nYoungest my number \"\n",
            " b\"ROMEO:\\nNo more than to your lordship prepailing\\nNorfetly against the gown; embrace the\\nservice, or seeming, makes me speak; I'll back you year imonthing\\nTo get him; but it is torment I thought,\\nWho is too hot that 'Farth of honour, in tears.\\nO, ye were a hundred one and pluck by, who\\nHow now, proud jost girrationably.\\n\\nGLOUCESTER:\\nSpring to my lord, Lether he shall not deserve to see\\nthe enry yet that threatened hin not, do you good\\nthe night by night to send; that na'timolima's made,\\nHas your mouths as steph Princely Buckingham,\\nSo doth me unking? But O, thy bid these\\nbeing sent for them satisfulty.\\n\\nFERDINAND:\\nWhere is the port of Richmond I am mean?\\n\\nBRUTUS:\\nHow dates made you a noble means are gone.\\n\\nCORIOLANUS:\\nA gold nor stirring on the sun,\\nAnd vex my brain makes she dutestion.\\n\\nKING RICHARD II:\\nMarry, I have found Bian:\\n'Tis love I thank you, good Belker Juliet,\\nAnd let him already that they say, it will.\\n\\nGLOUCESTER:\\nWhat! marry, may she; no doubt, he shall\\nNow know my meated base e\"\n",
            " b\"ROMEO:\\nHow now! what's the mid?\\n\\nJULIET:\\nAccursed be the table and friends that flies\\ntworn young cries 'DORCATH:\\nMy lords, hear me no money be an officer\\nTo use my letter, to the purpose twic;\\n'Tis labour of my life before the people\\nBut one hour in after reasons.\\n\\nBRUTUS:\\nEven in the hing that were\\nwere joys of charge into our wills,\\nWho for this action came from wnock,\\nAs if this lady's kind of barring reason\\nAnd again the break of vicady of a man\\nBut for some means to do the good in me\\nto him by the yon ado.\\n\\nKING RICHARD III:\\nStay, state away the thanks I make.\\n\\nBAPTISTA:\\nAnd I, with no day that do she proud?\\n'Tis I not live: thou art unball'd! Whom for the sick grable\\nBear your cause upon you all!\\n\\nFirst Gentleman:\\nWhat, ho! a thousand thanks?\\nWhat calamn's blemished see; which hast taste\\nThan crave the sky five flatterigns,\\nMore than all the strong slain in readinest:\\nAnd here it is to win thy dagger keeps?\\n\\nKING RICHARD III:\\nStanley, like a drain too much, make it up\\nThe prince that \"\n",
            " b\"ROMEO:\\nWhat stay had he trades the reind thou hast spoke,\\nHold Hermand at any galments, ours.\\nMadam, my heart will chide enter out.\\n\\nQUEEN MARGARET:\\nAnd welcome, gentlemen!\\nIf aught when grief defended mine, that our\\ncoundry becomes nature discerns to see me trod?\\n\\nFLORIZEL:\\nSleep you that you not?\\n\\nThird Citizen:\\nWe have pronounced my servant! do not shall\\nIf this swift way should be proclaim'd.\\n\\nBAPTISTA:\\nThe aduge already made in their queen\\nand more and like him, once more word than my success.\\nHis deeds shall live, for this will dee, but that\\nI claim hereafter than he is, hear me\\nStill, we do well set as I please; yet, if you are\\nlaid to Antigonus, to wail the throne;\\nWhere you is mean's death, good calling for no health,\\nAcquainted eggors of his trade.\\n\\nESCALUS:\\nNo more at our own report: since their infects me,\\nBecause you know--that you must do?\\n\\nCOMINIUS:\\nThe worn stands this to lime.\\n\\nBIONDELLO:\\nNay, be betode you I must: nothing but despair,\\nWhat fair world, die in the stard, for \"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.724074602127075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWYFkqI8HzrP",
        "outputId": "197b77ef-7626-4079-9e93-44d877e8a4a1"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7de162f35d20>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    }
  ]
}